{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "# import some common libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.modeling import build_model\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/06 23:16:49 d2.data.datasets.coco]: \u001b[0mLoading nucleus_cocoformat_poly2.json takes 1.48 seconds.\n",
      "\u001b[32m[12/06 23:16:49 d2.data.datasets.coco]: \u001b[0mLoaded 24 images in COCO format from nucleus_cocoformat_poly2.json\n"
     ]
    }
   ],
   "source": [
    "setup_logger()\n",
    "\n",
    "# register_coco_instances(\"nuclei_dataset\", {}, \"nucleus_cocoformat.json\", \"./train\")\n",
    "# register_coco_instances(\"nuclei_dataset\", {}, \"test.json\", \"./train\")\n",
    "register_coco_instances(\"nuclei_dataset\", {}, \"nucleus_cocoformat_poly2.json\", \"./train\")\n",
    "register_coco_instances(\"nuclei_dataset_val\", {}, \"nucleus_cocoformat_poly2.json\", \"./val\")\n",
    "metadata = MetadataCatalog.get(\"nuclei_dataset\")\n",
    "dataset_dicts = DatasetCatalog.get(\"nuclei_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
    "# from detectron2.data.datasets import register_coco_instances\n",
    "# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n",
    "# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path/to/image/dir\")\n",
    "\n",
    "from detectron2.structures import BoxMode\n",
    "def get_balloon_dicts(img_dir):\n",
    "    print(img_dir)\n",
    "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(imgs_anns.values()):\n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(img_dir, v[\"filename\"])\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "      \n",
    "        annos = v[\"regions\"]\n",
    "        objs = []\n",
    "        for _, anno in annos.items():\n",
    "            assert not anno[\"region_attributes\"]\n",
    "            anno = anno[\"shape_attributes\"]\n",
    "            px = anno[\"all_points_x\"]\n",
    "            py = anno[\"all_points_y\"]\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0,\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon/\" + d))\n",
    "    MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\n",
    "balloon_metadata = MetadataCatalog.get(\"balloon_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = get_balloon_dicts(\"balloon/train\")\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    # cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "    # cv2.imshow(\"out\", out.get_image()[:, :, ::-1])\n",
    "    cv2.imwrite('output.jpg', out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "# cfg.merge_from_file(\"./model/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
    "cfg.merge_from_file(model_zoo.get_config_file('COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'))\n",
    "# cfg.OUTPUT_DIR = \"./output\"\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url('COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml')\n",
    "# cfg.MODEL.WEIGHTS = os.path.join('model', \"model_final_Cascade.pkl\")\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# if you have pre-trained weight.\n",
    "cfg.DATASETS.TRAIN = (\"nuclei_dataset\",)\n",
    "# cfg.DATASETS.TRAIN = (\"balloon_train\",)\n",
    "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
    "cfg.DATALOADER.NUM_WORKERS = 0\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "# cfg.SOLVER.BASE_LR = 0.00025\n",
    "# cfg.SOLVER.BASE_LR = 0.00005\n",
    "cfg.SOLVER.BASE_LR = 0.000001\n",
    "# 4999 iterations seems good enough, but you can certainly train longer\n",
    "cfg.SOLVER.MAX_ITER = 100\n",
    "# faster, and good enough for this toy dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 8\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 1 classes (data, fig, hazelnut)\n",
    "# cfg.MODEL.DEVICE ='cuda'\n",
    "# cfg.INPUT.MASK_FORMAT = 'bitmask'\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# model = build_model(cfg)\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = build_model(cfg)\n",
    "\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import DatasetMapper   # the default mapper\n",
    "from detectron2.data import build_detection_train_loader\n",
    "\n",
    "dataloader = build_detection_train_loader(cfg,\n",
    "   mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "      T.Resize((1000, 1000))\n",
    "   ]), num_workers=0)\n",
    "\n",
    "from detectron2.engine import SimpleTrainer\n",
    "from detectron2.engine import HookBase\n",
    "from detectron2.solver import get_default_optimizer_params\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(get_default_optimizer_params(model),\n",
    "               lr=1e-4)\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "class CheckerHook(HookBase):\n",
    "  def after_step(self):\n",
    "    if self.trainer.iter % 100 == 0:\n",
    "      print(f\"Iteration {self.trainer.iter} complete\")\n",
    "\n",
    "\n",
    "\n",
    "trainer = SimpleTrainer(model, data_loader=dataloader, optimizer = optimizer) \n",
    "trainer.register_hooks([CheckerHook()])\n",
    "\n",
    "\n",
    "trainer.train(start_iter=0, max_iter=100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output\n",
      "\u001b[32m[12/07 17:11:42 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[12/07 17:11:43 d2.data.datasets.coco]: \u001b[0mLoaded 24 images in COCO format from nucleus_cocoformat_poly2.json\n",
      "\u001b[32m[12/07 17:11:43 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 24 images left.\n",
      "\u001b[32m[12/07 17:11:43 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[12/07 17:11:43 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[12/07 17:11:43 d2.data.common]: \u001b[0mSerializing 24 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/07 17:11:43 d2.data.common]: \u001b[0mSerialized dataset takes 22.23 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/07 17:11:43 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[12/07 17:11:43 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[12/07 17:11:48 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:11:53 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:11:58 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:12:04 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:12:13 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:12:18 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:12:26 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:12:31 d2.utils.events]: \u001b[0m eta: 0:00:56  iter: 19  total_loss: 0.8946  loss_cls: 0.06593  loss_box_reg: 0.137  loss_mask: 0.2508  loss_rpn_cls: 0.1062  loss_rpn_loc: 0.2316  time: 2.5581  data_time: 0.3983  lr: 1.9081e-07  max_mem: 6026M\n",
      "\u001b[32m[12/07 17:12:32 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:12:38 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:12:47 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:12:55 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:01 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:06 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:12 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:17 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:21 d2.utils.events]: \u001b[0m eta: 0:00:40  iter: 39  total_loss: 0.9085  loss_cls: 0.06945  loss_box_reg: 0.2208  loss_mask: 0.2679  loss_rpn_cls: 0.1098  loss_rpn_loc: 0.2264  time: 2.5395  data_time: 0.4272  lr: 3.9061e-07  max_mem: 6026M\n",
      "\u001b[32m[12/07 17:13:26 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:30 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:36 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:42 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:47 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:51 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:13:59 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:04 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:08 d2.utils.events]: \u001b[0m eta: 0:00:26  iter: 59  total_loss: 0.9719  loss_cls: 0.1078  loss_box_reg: 0.2519  loss_mask: 0.2494  loss_rpn_cls: 0.1233  loss_rpn_loc: 0.229  time: 2.4626  data_time: 0.3907  lr: 5.9041e-07  max_mem: 6026M\n",
      "\u001b[32m[12/07 17:14:10 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:20 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:23 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:29 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:37 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:40 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:49 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:53 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:14:58 d2.utils.events]: \u001b[0m eta: 0:00:13  iter: 79  total_loss: 0.93  loss_cls: 0.06185  loss_box_reg: 0.2846  loss_mask: 0.2597  loss_rpn_cls: 0.132  loss_rpn_loc: 0.2327  time: 2.4710  data_time: 0.4194  lr: 7.9021e-07  max_mem: 6026M\n",
      "\u001b[32m[12/07 17:14:59 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:15:06 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:15:13 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:15:18 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:15:23 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:15:30 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:15:36 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x0000023C07A13670> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/07 17:15:40 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 99  total_loss: 0.9801  loss_cls: 0.1074  loss_box_reg: 0.2592  loss_mask: 0.3011  loss_rpn_cls: 0.1269  loss_rpn_loc: 0.2239  time: 2.3976  data_time: 0.3924  lr: 9.9001e-07  max_mem: 6026M\n",
      "\u001b[32m[12/07 17:15:40 d2.engine.hooks]: \u001b[0mOverall training speed: 98 iterations in 0:03:54 (2.3976 s / it)\n",
      "\u001b[32m[12/07 17:15:40 d2.engine.hooks]: \u001b[0mTotal training time: 0:03:55 (0:00:00 on hooks)\n"
     ]
    }
   ],
   "source": [
    "print(cfg.OUTPUT_DIR\n",
    ")\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)  # build output folder\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 20112), started 1 day, 1:29:22 ago. (Use '!kill 20112' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-665f66c42dd16c6f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-665f66c42dd16c6f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "# !kill 20112\n",
    "%reload_ext tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in random.sample(dataset_dicts, 1):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    # cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "    # cv2.imshow(\"out\", out.get_image()[:, :, ::-1])\n",
    "    cv2.imwrite('output_poly.jpg', out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/07 17:16:04 d2.data.datasets.coco]: \u001b[0mLoaded 24 images in COCO format from nucleus_cocoformat_poly2.json\n",
      "\u001b[32m[12/07 17:16:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[12/07 17:16:04 d2.data.common]: \u001b[0mSerializing 24 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/07 17:16:04 d2.data.common]: \u001b[0mSerialized dataset takes 22.23 MiB\n",
      "\u001b[32m[12/07 17:16:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 24 batches\n",
      "\u001b[32m[12/07 17:16:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/24. Dataloading: 0.0993 s/iter. Inference: 0.1106 s/iter. Eval: 0.2222 s/iter. Total: 0.4321 s/iter. ETA=0:00:05\n",
      "\u001b[32m[12/07 17:16:15 d2.evaluation.evaluator]: \u001b[0mInference done 23/24. Dataloading: 0.1175 s/iter. Inference: 0.1102 s/iter. Eval: 0.2203 s/iter. Total: 0.4480 s/iter. ETA=0:00:00\n",
      "\u001b[32m[12/07 17:16:15 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:08.568224 (0.450959 s / iter per device, on 1 devices)\n",
      "\u001b[32m[12/07 17:16:15 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:02 (0.110259 s / iter per device, on 1 devices)\n",
      "\u001b[32m[12/07 17:16:15 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[12/07 17:16:15 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output\\coco_instances_results.json\n",
      "\u001b[32m[12/07 17:16:15 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[12/07 17:16:15 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.30 seconds.\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.100\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.158\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.117\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.329\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.107\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.368\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:------:|:-----:|\n",
      "| 9.992 | 15.828 | 11.689 | 9.010 | 32.909 |  nan  |\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.49 seconds.\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.099\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.158\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.088\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.351\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.105\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.366\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:------:|:-----:|\n",
      "| 9.895 | 15.812 | 11.756 | 8.826 | 35.131 |  nan  |\n",
      "\u001b[32m[12/07 17:16:16 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "OrderedDict([('bbox', {'AP': 9.992496542225398, 'AP50': 15.827701796821502, 'AP75': 11.688994464912735, 'APs': 9.009681936196797, 'APm': 32.908725202476916, 'APl': nan}), ('segm', {'AP': 9.895471012718064, 'AP50': 15.811862627131484, 'AP75': 11.755990157451865, 'APs': 8.826276280138034, 'APm': 35.13076450855795, 'APl': nan})])\n"
     ]
    }
   ],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"nuclei_dataset_val\", output_dir=\"./output\")\n",
    "val_loader = build_detection_test_loader(cfg, \"nuclei_dataset_val\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7df3907372d3e53ad8e301f24b7bc5202239144b65b0e5864a0862ce56cea728"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('fb_detectron2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
